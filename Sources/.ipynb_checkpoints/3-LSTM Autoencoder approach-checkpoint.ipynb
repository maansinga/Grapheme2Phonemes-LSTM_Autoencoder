{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import re\n",
    "from keras import objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    words = []\n",
    "    with open(\"words.txt\") as fi:\n",
    "        line = fi.readline()\n",
    "        while(len(line)!=0):\n",
    "            datapoint = line.split(' ')\n",
    "            phon = datapoint[1].split('.')\n",
    "            phon[-1] = phon[-1][:-1]\n",
    "            words.append([datapoint[0], phon])\n",
    "            line = fi.readline()\n",
    "            \n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['aback', ['AH', 'B', 'AE', 'K']],\n",
       " ['abandon', ['AH', 'B', 'AE', 'N', 'D', 'AH', 'N']],\n",
       " ['abandoned', ['AH', 'B', 'AE', 'N', 'D', 'AH', 'N', 'D']],\n",
       " ['abandoning', ['AH', 'B', 'AE', 'N', 'D', 'AH', 'N', 'IH', 'NG']],\n",
       " ['abandons', ['AH', 'B', 'AE', 'N', 'D', 'AH', 'N', 'Z']]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xraw = [k[0] for  k in data]\n",
    "Yraw = [k[1] for  k in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aback', 'abandon', 'abandoned', 'abandoning', 'abandons']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xraw[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 22468, 'uncharacteristically')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xrawlens = np.array([len(x) for x in Xraw])\n",
    "Xrawlens.max(), Xrawlens.argmax(), Xraw[Xrawlens.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['AH', 'B', 'AE', 'K'],\n",
       " ['AH', 'B', 'AE', 'N', 'D', 'AH', 'N'],\n",
       " ['AH', 'B', 'AE', 'N', 'D', 'AH', 'N', 'D'],\n",
       " ['AH', 'B', 'AE', 'N', 'D', 'AH', 'N', 'IH', 'NG'],\n",
       " ['AH', 'B', 'AE', 'N', 'D', 'AH', 'N', 'Z']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yraw[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,\n",
       " 4046,\n",
       " ['K',\n",
       "  'AA',\n",
       "  'M',\n",
       "  'P',\n",
       "  'AA',\n",
       "  'R',\n",
       "  'T',\n",
       "  'M',\n",
       "  'EH',\n",
       "  'N',\n",
       "  'T',\n",
       "  'AH',\n",
       "  'L',\n",
       "  'AY',\n",
       "  'Z',\n",
       "  'D'],\n",
       " 'compartmentalized')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yrawlens = np.array([len(y) for y in Yraw])\n",
    "Yrawlens.max(), Yrawlens.argmax(), Yraw[Yrawlens.argmax()], Xraw[Yrawlens.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2tensor(word, wordmaxlen=24):\n",
    "    # print(word)\n",
    "    word = '`' + word + '{'\n",
    "    word2tensor.dummy_char_vec = np.array([0]*28)\n",
    "    wordvec_array = []\n",
    "    padding = wordmaxlen - len(word)\n",
    "    for eachcharacter in word:\n",
    "        tempchar = word2tensor.dummy_char_vec.copy()\n",
    "        tempchar[ord(eachcharacter) - ord('`')] = 1\n",
    "        wordvec_array.append(tempchar)\n",
    "        \n",
    "    for i in range(padding):\n",
    "        wordvec_array.append(word2tensor.dummy_char_vec.copy())\n",
    "    return np.array(wordvec_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def word2tensor(word, wordmaxlen=24):\n",
    "#     # print(word)\n",
    "#     word = '`' + word + '{'\n",
    "#     wordvec_array = []\n",
    "#     padding = wordmaxlen - len(word)\n",
    "#     for eachcharacter in word:\n",
    "#         tempchar = (ord(eachcharacter) - ord('`'))/27.0\n",
    "#         wordvec_array.append(tempchar)\n",
    "        \n",
    "#     for i in range(padding):\n",
    "#         wordvec_array.append((ord('{') - ord('`'))/27.0)\n",
    "#     return np.array(wordvec_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = word2tensor(\"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]]), (24, 28))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result, result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting words to tensor representations and save them for fast loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24167, 24, 28)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = np.array([word2tensor(w) for w in Xraw])\n",
    "words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_shape = words.shape\n",
    "np.savez_compressed('words2vec.npz', words = words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.load('words2vec.npz')\n",
    "# words = data['words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phonemes: There are 39 phonemes, as shown below:\n",
    "`\n",
    "AA     odd     AA D        |   AE      at      AE T      \n",
    "AH      hut     HH AH T     |   AO      ought   AO T     \n",
    "AW      cow     K AW        |   AY      hide    HH AY D  \n",
    "B       be      B IY        |   CH      cheese  CH IY Z  \n",
    "D       dee     D IY        |   DH      thee    DH IY    \n",
    "EH      Ed      EH D        |   ER      hurt    HH ER T  \n",
    "EY      ate     EY T        |   F       fee     F IY     \n",
    "G       green   G R IY N    |   HH      he      HH IY    \n",
    "IH      it      IH T        |   IY      eat     IY T     \n",
    "JH      gee     JH IY       |   K       key     K IY     \n",
    "L       lee     L IY        |   M       me      M IY     \n",
    "N       knee    N IY        |   NG      ping    P IH NG  \n",
    "OW      oat     OW T        |   OY      toy     T OY     \n",
    "P       pee     P IY        |   R       read    R IY D   \n",
    "S       sea     S IY        |   SH      she     SH IY    \n",
    "T       tea     T IY        |   TH      theta   TH EY T AH \n",
    "UH      hood    HH UH D     |   UW      two     T UW     \n",
    "V       vee     V IY        |   W       we      W IY     \n",
    "Y       yield   Y IY L D    |   Z       zee     Z IY     \n",
    "ZH      seizure S IY ZH ER  | \n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_map = {\n",
    "    '\\s': 0, # Phoneme starting marker\n",
    "    'AA': 1,\n",
    "    'AE': 2,\n",
    "    'AH': 3,\n",
    "    'AO': 4,\n",
    "    'AW': 5,\n",
    "    'AY': 6,\n",
    "    'B': 7,\n",
    "    'CH': 8,\n",
    "    'D': 9,\n",
    "    'DH': 10,\n",
    "    'EH': 11,\n",
    "    'ER': 12,\n",
    "    'EY': 13,\n",
    "    'F': 14,\n",
    "    'G': 15,\n",
    "    'HH': 16,\n",
    "    'IH': 17,\n",
    "    'IY': 18,\n",
    "    'JH': 19,\n",
    "    'K': 20,\n",
    "    'L': 21,\n",
    "    'M': 22,\n",
    "    'N': 23,\n",
    "    'NG': 24,\n",
    "    'OW': 25,\n",
    "    'OY': 26,\n",
    "    'P': 27,\n",
    "    'R': 28,\n",
    "    'S': 29,\n",
    "    'SH': 30,\n",
    "    'T': 31,\n",
    "    'TH': 32,\n",
    "    'UH': 33,\n",
    "    'UW': 34,\n",
    "    'V': 35,\n",
    "    'W': 36,\n",
    "    'Y': 37,\n",
    "    'Z': 38,\n",
    "    'ZH': 39,\n",
    "    '\\e': 40 # Phoneme termination marker\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phonemes2tensor(phonemes, phonemesmaxlen=20):\n",
    "    # print(word)\n",
    "    phonemes = ['\\s'] + phonemes + ['\\e']\n",
    "    phonemes2tensor.dummy_phoneme_vec = np.array([0]*41) # dummy phonetic symbol as stop\n",
    "    phonemesvec_array = []\n",
    "    padding = phonemesmaxlen - len(phonemes)\n",
    "    for eachphoneme in phonemes:\n",
    "        tempphoneme = phonemes2tensor.dummy_phoneme_vec.copy()\n",
    "        tempphoneme[phoneme_map[eachphoneme]] = 1\n",
    "        phonemesvec_array.append(tempphoneme)\n",
    "        \n",
    "    for i in range(padding):\n",
    "        phonemesvec_array.append(phonemes2tensor.dummy_phoneme_vec.copy())\n",
    "    return np.array(phonemesvec_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def phonemes2tensor(phonemes, phonemesmaxlen=20):\n",
    "#     # print(word)\n",
    "#     phonemes = ['\\s'] + phonemes + ['\\e']\n",
    "#     phonemesvec_array = []\n",
    "#     padding = phonemesmaxlen - len(phonemes)\n",
    "#     for eachphoneme in phonemes:\n",
    "#         tempphoneme = phoneme_map[eachphoneme]\n",
    "#         phonemesvec_array.append(tempphoneme/40.0)\n",
    "        \n",
    "#     for i in range(padding):\n",
    "#         phonemesvec_array.append(1.0)\n",
    "#     return np.array(phonemesvec_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), (20, 41))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = phonemes2tensor(Yraw[12])\n",
    "result, result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24167, 20, 41)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonemes = np.array([phonemes2tensor(p) for p in Yraw])\n",
    "phonemes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_shape = words.shape\n",
    "np.savez_compressed('phonemes2vec.npz', phonemes = words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.load('words2vec.npz')\n",
    "# phonemes = data['phonemes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 24, 28)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2_lstm1 (LSTM)            (None, 64)           23808       input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm1_2_dense1 (Dense)          (None, 128)          8320        input_2_lstm1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense1_2_dense2 (Dense)         (None, 128)          8320        input_2_lstm1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense2_2_lambda1 (Lambda)       (None, 128)          0           lstm1_2_dense1[0][0]             \n",
      "                                                                 dense1_2_dense2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda1_2_repeat (RepeatVector) (None, 24, 128)      0           dense2_2_lambda1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dh_lstm (LSTM)                  (None, 24, 64)       49408       lambda1_2_repeat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "du_lstm (LSTM)                  (None, 24, 28)       10416       dh_lstm[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 100,272\n",
      "Trainable params: 100,272\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/64\n",
      "10208/24167 [===========>..................] - ETA: 55s - loss: 0.0135"
     ]
    }
   ],
   "source": [
    "timesteps = 24\n",
    "_timesteps = 20\n",
    "input_dim = 28\n",
    "output_dim = 41\n",
    "intermediate_dim = 64\n",
    "latent_dim = 128\n",
    "batch_size = 32\n",
    "epsilon_std = 1.0\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "with tf.device(\"GPU:1\"):\n",
    "    x = keras.Input(shape=(timesteps, input_dim), name='input')\n",
    "\n",
    "    # LSTM encoding\n",
    "    h = keras.layers.LSTM((intermediate_dim), name='input_2_lstm1')(x)\n",
    "\n",
    "    # VAE Z layer\n",
    "    z_mean = keras.layers.Dense(latent_dim, name='lstm1_2_dense1')(h)\n",
    "    z_log_sigma = keras.layers.Dense(latent_dim, name='dense1_2_dense2')(h)\n",
    "\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_sigma = args\n",
    "        epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                                  mean=0., stddev=epsilon_std)\n",
    "        return z_mean + z_log_sigma * epsilon\n",
    "\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    # so you could write `Lambda(sampling)([z_mean, z_log_sigma])`\n",
    "    z = keras.layers.Lambda(sampling, output_shape=(latent_dim,),name='dense2_2_lambda1')([z_mean, z_log_sigma])\n",
    "\n",
    "    # decoded LSTM layer\n",
    "    decoder_h = keras.layers.LSTM(intermediate_dim, return_sequences=True, name='dh_lstm')\n",
    "    decoder_mean = keras.layers.LSTM(input_dim, return_sequences=True, name='du_lstm')\n",
    "\n",
    "    h_decoded = keras.layers.RepeatVector(timesteps, name='lambda1_2_repeat')(z)\n",
    "    h_decoded = decoder_h(h_decoded)\n",
    "\n",
    "    # decoded layer\n",
    "    x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "    # end-to-end autoencoder\n",
    "    vae = keras.Model(x, x_decoded_mean)\n",
    "\n",
    "    # encoder, from inputs to latent space\n",
    "    encoder = keras.Model(x, z_mean)\n",
    "\n",
    "    # generator, from latent space to reconstructed inputs\n",
    "    decoder_input = keras.layers.Input(shape=(latent_dim,))\n",
    "\n",
    "    _h_decoded = keras.layers.RepeatVector(timesteps)(decoder_input)\n",
    "    _h_decoded = decoder_h(_h_decoded)\n",
    "\n",
    "    _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "    generator = keras.Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "    def vae_loss(x, x_decoded_mean):\n",
    "        xent_loss = objectives.mse(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma))\n",
    "        loss = xent_loss + kl_loss\n",
    "        return loss\n",
    "\n",
    "    vae.compile(optimizer='rmsprop', loss=vae_loss)\n",
    "\n",
    "    vae.summary()\n",
    "\n",
    "\n",
    "    vae.fit(x=words, y=words, epochs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
